{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This Notebook is mainly for exploring the data and analysing Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do some simple Data Exploration here, since the data looks like it is from Grab's Data Warehouse \n",
    "# and there is not much cleaning to be done, only transformation into input matrix for the LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./training.csv\")\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking how many data entries in total\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values in the data\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the day range\n",
    "df.sort_values(by='day',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the data points for each of the geohashes\n",
    "df.geohash6.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the demand range\n",
    "df.sort_values(by='demand',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can see that the all the target demand values are already normalised between 0 and 1\n",
    "## There is no null values in the dataset\n",
    "## The given data is from day 1 until day 61\n",
    "## Data transformation into model input is done in the other Jupyter Notebook\n",
    "## The following section is used to analyse the model during training and troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the training and validation error on the top 20 models\n",
    "# to ensure that all the models are training properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loop through the sorted models\n",
    "file = open(\"./All_Trained_Models/Result_Store_Sorted.json\", 'r')\n",
    "sorted_model = json.loads(file.read())\n",
    "\n",
    "fig = figure()\n",
    "fig.set_figheight(15)\n",
    "fig.set_figwidth(15)\n",
    "\n",
    "k = 1\n",
    "#Checking only the top 20 models\n",
    "for i in range(20):\n",
    "    i = i + 1\n",
    "    s = sorted_model[str(i)]['Model']    \n",
    "    history_dict = json.load(open(\"./All_Trained_Models/{}_model_history.json\".format(s), 'r'))\n",
    "    a = history_dict['loss']\n",
    "    b = history_dict['val_loss']\n",
    "\n",
    "    subplot(5,4,k)    \n",
    "    title('({})'.format(s))\n",
    "    k+=1\n",
    "    plot(a)\n",
    "    plot(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the model output in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(T5_actual,T5_pred): \n",
    "    return np.sqrt(np.mean((T5_actual-T5_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_matrix = np.load('./test_matrix.npy')\n",
    "\n",
    "T_shift = 0         # Amount to shift the time-series input data\n",
    "Day_Feed = 13       # Predict T+5 from x days\n",
    "day_feed = Day_Feed*24*4\n",
    "\n",
    "#Get the sorted model info\n",
    "file = open(\"./All_Trained_Models/Result_Store_Sorted.json\", 'r')\n",
    "sorted_model = json.loads(file.read())\n",
    "\n",
    "#Checking the best model\n",
    "s = sorted_model[str(1)]['Model']\n",
    "\n",
    "# load json and create model\n",
    "json_file = open(\"./All_Trained_Models/{}_model.json\".format(s), 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights((\"./All_Trained_Models/{}_model.h5\".format(s)))\n",
    "\n",
    "\n",
    "input_set = input_matrix[:, T_shift:T_shift+day_feed,:,:,:]\n",
    "target_set = input_matrix[:, T_shift:T_shift + day_feed + 5,:,:,:]\n",
    "pred_out = input_set[0][:,:,:,:]\n",
    "\n",
    "# Make predictions using the model\n",
    "for i in range(5):\n",
    "    new_pred = loaded_model.predict(pred_out[np.newaxis, ::, ::, ::, ::])\n",
    "    new = new_pred[::, -1, ::, ::, ::]\n",
    "    pred_out = np.concatenate((pred_out, new), axis=0)\n",
    "\n",
    "T5_pred = pred_out[-5:,:,:,:]                    \n",
    "T5_actual = target_set[0][-5:,:,:,:]  \n",
    "\n",
    "# Plot the prediction output and show the target output\n",
    "fig, axs = subplots(nrows=5, ncols=2, figsize=(15, 30),\n",
    "    subplot_kw={'xlabel': 'lattitude', 'ylabel': 'longitude'})\n",
    "\n",
    "for ax, count, index in zip(axs.flat,range(10),range(10)):\n",
    "\n",
    "    index = int(index/2)\n",
    "\n",
    "    T5_actual_plot = np.reshape(T5_actual[index,:,:,:],(36,46))\n",
    "    T5_pred_plot = np.reshape(T5_pred[index,:,:,:],(36,46))\n",
    "\n",
    "    if count%2 == 0:\n",
    "        ax.imshow(T5_pred_plot)\n",
    "        ax.set_title('Prediction (T+{})'.format(index+1))\n",
    "    else:\n",
    "        ax.imshow(T5_actual_plot)\n",
    "        ax.set_title('Actual (T+{})'.format(index+1))\n",
    "\n",
    "print('Showing Preditions vs Actual for Model {}'.format(s))\n",
    "# tight_layout()\n",
    "# show()\n",
    "\n",
    "rmse = get_rmse(T5_actual,T5_pred)\n",
    "\n",
    "print('RMSE for {} model is {}'.format(s,rmse))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the numerical value of the model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The last 5 values are always prediction values from the model\n",
    "T_plus_check = 15 # check end of how many timesteps \n",
    "df_test = pd.read_csv(\"./test.csv\")\n",
    "coords_df = pd.read_csv(\"./coords_mapping.csv\")\n",
    "\n",
    "df_out = df_test\n",
    "\n",
    "# Removing time series and keep only the unique geohashes\n",
    "df_out = pd.DataFrame({'geohash6' : df_out['geohash6'].unique()})\n",
    "# merge on test dataset geohash with coords_df, drop if no geohash match\n",
    "df_out = pd.merge(df_out, coords_df, how = 'left', on = ['geohash6'])\n",
    "\n",
    "#Prediction for T+0 to T_plus_check\n",
    "for i in range(T_plus_check):\n",
    "\n",
    "    Matrix = np.reshape(pred_out[i,:,:,:],(36,46))  \n",
    "    print('Mapping Prediction Output into DataFrame: {} of {}.'.format(i,T_plus_check), end='\\r')\n",
    "    df_out['T+{}'.format(i)] = pd.Series()\n",
    "    long_group = df_out.groupby('long_index')\n",
    "    for long_val, long_dfs in long_group:\n",
    "        lat_group = long_dfs.groupby('lat_index')         \n",
    "        for lat_val, lat_dfs in lat_group:\n",
    "            index = lat_dfs.index.tolist()\n",
    "            df_out.set_value(index, 'T+{}'.format(i),Matrix[long_val,lat_val])\n",
    "df_out = df_out.drop(columns = ['lat','long','lat_err','long_err','lat_index','long_index',])\n",
    "df_out.head(40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
