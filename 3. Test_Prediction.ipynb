{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Running this code requires the following files and folders:\n",
    "#     1. \"./test.csv\"\n",
    "#     2. \"./coords_mapping.csv\"\n",
    "#     3. \"./All_Trained_Models/Result_Store_Sorted.json\"\n",
    "#     4. \"./Top_20_Models/\"     *with the top 20 models inside this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the test dataset and the coordinates mapping file\n",
    "df_test = pd.read_csv(\"./test.csv\")\n",
    "coords_df = pd.read_csv(\"./coords_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Pre-process the data into time-series data first\n",
    "#Define timestamp to be have one day cycle, ie 24hrs x 4 (15 min buckets) = 96\n",
    "df = df_test\n",
    "\n",
    "# Reformat timestamp\n",
    "ts_df = pd.DataFrame({'timestamp' : df['timestamp'].unique()})\n",
    "ts_df[['h','m']]=ts_df['timestamp'].apply(lambda cell: pd.Series(cell.split(\":\")))\n",
    "ts_df['h'] = ts_df['h'].apply(lambda cell:int(cell)*4)\n",
    "ts_df['m'] = ts_df['m'].apply(lambda cell:int(cell)/15)\n",
    "ts_df['new_ts']= ts_df['h'] + ts_df['m']\n",
    "df = df.merge(ts_df.drop(['h','m'],axis=1), on ='timestamp', how='inner')\n",
    "\n",
    "# We apply coords_df from csv to ensure the geohash to input matrix mapping is correct\n",
    "df = df.merge(coords_df.drop(columns=['lat','long','lat_err','long_err']), on='geohash6', how='left')\n",
    "df = df.drop(columns = ['geohash6','timestamp'])\n",
    "\n",
    "# Merge 'day' with 'timestamp' to generate a continous time-series\n",
    "df['time-series'] = df['day'].apply(lambda cell: (cell-1)*24*4) + df['new_ts']\n",
    "df['time-series'] = df['time-series'] - df['time-series'].min()\n",
    "df['time-series'] = df['time-series'].astype(int)\n",
    "\n",
    "# Drop unnecessary columns and sort by time and longitude\n",
    "df = df.drop(columns = ['day','new_ts'])\n",
    "df = df.sort_values(by=['time-series','long_index'],ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into matrix of shape [n_samples, time-series, longtitude, lattitude, depth] to feed into ConvLSTM for training\n",
    "n_samples = 1\n",
    "n_frames = df['time-series'].max().astype(int) + 1\n",
    "long = df['long_index'].max().astype(int) + 1\n",
    "lat = df['lat_index'].max().astype(int) + 1\n",
    "\n",
    "input_matrix = np.zeros((n_samples, n_frames, long, lat, 1), dtype=np.float)\n",
    "\n",
    "# Create an array of zeros to fill in Dataframe where there are missing data points\n",
    "a = np.zeros((46,4), dtype=np.int)\n",
    "a[:, 1] =  np.linspace(0, 45, 46,dtype=int)\n",
    "df_zeros = pd.DataFrame({'demand':a[:,0],'lat_index':a[:,1],'long_index':a[:,2],'time-series':a[:,3]})\n",
    "\n",
    "#time_val here is the values from 'time-series', also the index number for the time   \n",
    "time_grouped = df.groupby('time-series')\n",
    "for time_val, time_dfs in time_grouped:    \n",
    "    long_group = time_dfs.groupby('long_index')\n",
    "    print('Mapping Data into 2D: {} of {}.'.format(time_val+1,n_frames), end='\\r')    \n",
    "    #long_val here is the values from 'long_index', also the index number for the row\n",
    "    for long_val, long_dfs in long_group:        \n",
    "        long_dfs = long_dfs.append(df_zeros)\n",
    "        long_dfs = long_dfs.groupby('lat_index').max().reset_index()\n",
    "        b = long_dfs['demand'].values        \n",
    "        c = np.matrix(b.tolist())\n",
    "        input_matrix[n_samples-1,time_val,long_val,:,0]=c.reshape(46)\n",
    "        \n",
    "# Prepare the input and output matrix set for training\n",
    "input_set = input_matrix[:,:-1,:,:,:]\n",
    "target_set = input_matrix[:,1:,:,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the RMSE of all the Top 20 models on Test Data rigorously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(T5_actual,T5_pred): \n",
    "    return np.sqrt(np.mean((T5_actual-T5_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open(\"./All_Trained_Models/Result_Store_Sorted.json\", 'r')\n",
    "sorted_model = json.loads(file.read())\n",
    "\n",
    "T_shift = [0,48,96,144,187]   # 0 - 187, shifts the test data set\n",
    "Day_Feed = [1,3,7,10,12] #Predict T+5 from x days\n",
    "Overall_min = 1\n",
    "\n",
    "Result_Store = {}\n",
    "\n",
    "for i in range(20):\n",
    "    i = i + 1\n",
    "    s = sorted_model[str(i)]['Model']\n",
    "    \n",
    "    #extract the 'day' value from the file title\n",
    "    day = float(s[s.find('layer')+len('layer'):s.rfind('day')])\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open(\"./Top_20_Models/{}_model.json\".format(sorted_model[str(i)]['Model']), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into model\n",
    "    loaded_model.load_weights((\"./Top_20_Models/{}_model.h5\".format(sorted_model[str(i)]['Model'])))\n",
    "    # print(\"Loaded model from disk\")  \n",
    "    \n",
    "    RMSE_Day_total = 0\n",
    "    RMSE_Day_total_baseline = 0 # for baseline RMSE----------------\n",
    "    Max_Day_rmse = 0\n",
    "    Min_Day_rmse = 1\n",
    "\n",
    "    Day_all_store = {}\n",
    "    for day in Day_Feed:\n",
    "        RMSE_T_shift_total = 0\n",
    "        Max_T_shift_rmse = 0\n",
    "        Min_T_shift_rmse = 1\n",
    "        day_feed = day*24*4\n",
    "\n",
    "        for a in T_shift:\n",
    "            input_set = input_matrix[:, a : a + day_feed,:,:,:]\n",
    "            target_set = input_matrix[:, a : a + day_feed + 5,:,:,:]\n",
    "            train_14_day = input_set[0][:,:,:,:]\n",
    "\n",
    "            for k in range(5):\n",
    "                new_pred = loaded_model.predict(train_14_day[np.newaxis, ::, ::, ::, ::])\n",
    "                new = new_pred[::, -1, ::, ::, ::]\n",
    "                train_14_day = np.concatenate((train_14_day, new), axis=0)\n",
    "\n",
    "            T5_pred = train_14_day[-5:,:,:,:]\n",
    "            T5_actual = target_set[0][-5:,:,:,:]                    \n",
    "            rmse = get_rmse(T5_actual,T5_pred)  \n",
    "\n",
    "            # ----------------------------for baseline RMSE----------------\n",
    "            T5_baseline = train_14_day[-6:-5,:,:,:]\n",
    "            baseline = train_14_day[-6:-5,:,:,:]\n",
    "            for k in range(4):                        \n",
    "                T5_baseline = np.concatenate((T5_baseline, baseline), axis=0)\n",
    "            baseline_rmse = get_rmse(T5_actual,T5_baseline)  \n",
    "            # ----------------------------for baseline RMSE----------------\n",
    "\n",
    "            if rmse > Max_T_shift_rmse:\n",
    "                Max_T_shift_rmse = rmse\n",
    "\n",
    "            if rmse < Min_T_shift_rmse:\n",
    "                Min_T_shift_rmse = rmse                      \n",
    "\n",
    "            if rmse > Max_Day_rmse:\n",
    "                Max_Day_rmse = rmse\n",
    "\n",
    "            if rmse < Min_Day_rmse:\n",
    "                Min_Day_rmse = rmse\n",
    "                \n",
    "\n",
    "            RMSE_T_shift_total += rmse\n",
    "            RMSE_Day_total += rmse    \n",
    "            RMSE_Day_total_baseline += baseline_rmse  # for baseline RMSE----------------\n",
    "        RMSE_T_shift_avg = RMSE_T_shift_total/len(T_shift)\n",
    "\n",
    "        Day_store = {}\n",
    "        Day_store['Avg'] = RMSE_T_shift_avg\n",
    "        Day_store['Max'] = Max_T_shift_rmse\n",
    "        Day_store['Min'] = Min_T_shift_rmse\n",
    "\n",
    "        Day_all_store[day] = Day_store\n",
    "\n",
    "        Result_Store['{}'.format(sorted_model[str(i)]['Model'])] = Day_all_store\n",
    "    RMSE_Day_total_avg = RMSE_Day_total/(len(T_shift)*len(Day_Feed))\n",
    "    RMSE_Day_total_baseline_avg = RMSE_Day_total_baseline/(len(T_shift)*len(Day_Feed)) # for baseline RMSE----------------\n",
    "\n",
    "    Model_store = Day_all_store\n",
    "    Model_store['Avg'] = RMSE_Day_total_avg\n",
    "    Model_store['Max'] = Max_Day_rmse\n",
    "    Model_store['Min'] = Min_Day_rmse            \n",
    "\n",
    "    Result_Store['{}'.format(sorted_model[str(i)]['Model'])] = Model_store  \n",
    "\n",
    "    print(Result_Store)\n",
    "    print(\"Baseline RMSE is: {}\".format(RMSE_Day_total_baseline_avg))\n",
    "# save Result_Store to JSON\n",
    "json.dump(Result_Store, open(\"./Top_20_Models/Result_Score.json\", 'w'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Result_Store\n",
    "Result_Store = json.load(open(\"./Top_20_Models/Result_Score.json\", 'r')) \n",
    "\n",
    "Result_Store_Sorted = {}\n",
    "i = 1\n",
    "for key, value in sorted(Result_Store.items(), key=lambda tup: (tup[1]['Avg'])):\n",
    "    Model_dict = {}\n",
    "    Model_dict['Model'] = key\n",
    "    Model_dict['Avg'] = value['Avg']\n",
    "    Model_dict['Max'] = value['Max']\n",
    "    Model_dict['Min'] = value['Min']\n",
    "    Result_Store_Sorted['{}'.format(i)] = Model_dict  \n",
    "    i+=1\n",
    "\n",
    "json.dump(Result_Store_Sorted, open(\"./Top_20_Models/Result_Store_Sorted.json\", 'w'))\n",
    "\n",
    "print(Result_Store_Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble prediction from Top Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./Top_20_Models/Result_Store_Sorted.json\", 'r')\n",
    "sorted_model = json.loads(file.read())\n",
    "\n",
    "T5_pred_ensemble = np.zeros([6,36,46,1])\n",
    "\n",
    "# Take Top 1 models only\n",
    "# Can modify this value to take average of top x Models\n",
    "# In this case we only use the top model to give the prediction output\n",
    "Top_Model = 1\n",
    "for i in range(Top_Model):\n",
    "    i = i + 1\n",
    "    s = sorted_model[str(i)]['Model']\n",
    "    \n",
    "    #extract the 'day' value from the file title\n",
    "    day = float(s[s.find('layer')+len('layer'):s.rfind('day')])\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open(\"./Top_20_Models/{}_model.json\".format(sorted_model[str(i)]['Model']), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into model\n",
    "    loaded_model.load_weights((\"./Top_20_Models/{}_model.h5\".format(sorted_model[str(i)]['Model'])))\n",
    "    # print(\"Loaded model from disk\")\n",
    "    \n",
    "    input_set = input_matrix[:,:,:,:,:]\n",
    "    train_14_day = input_set[0][:,:,:,:]\n",
    "    \n",
    "    #Predicting T+1 to T+5\n",
    "    for k in range(5):\n",
    "        new_pred = loaded_model.predict(train_14_day[np.newaxis, ::, ::, ::, ::])\n",
    "        new = new_pred[::, -1, ::, ::, ::]\n",
    "        train_14_day = np.concatenate((train_14_day, new), axis=0)\n",
    "        \n",
    "    T5_pred = train_14_day[-6:,:,:,:]\n",
    "    T5_pred_ensemble += T5_pred\n",
    "\n",
    "T5_pred_out = np.divide(T5_pred_ensemble,Top_Model)     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat Prediction Output into Dataframe and output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_test\n",
    "\n",
    "# Keep only the unique geohashes\n",
    "df_out = pd.DataFrame({'geohash6' : df_out['geohash6'].unique()})\n",
    "\n",
    "# merge on test dataset geohash with coords_df, drop if no geohash match\n",
    "df_out = pd.merge(df_out, coords_df, how = 'left', on = ['geohash6'])\n",
    "\n",
    "#Prediction for T+0 to T+5\n",
    "for i in range(6):\n",
    "    Matrix = np.reshape(T5_pred_out[i,:,:,:],(36,46))  \n",
    "    print('Mapping Data into Matrix: {} of {}.'.format(i,5), end='\\r')\n",
    "    df_out['T+{}'.format(i)] = pd.Series()\n",
    "    long_group = df_out.groupby('long_index')\n",
    "    for long_val, long_dfs in long_group:\n",
    "        lat_group = long_dfs.groupby('lat_index')         \n",
    "        for lat_val, lat_dfs in lat_group:\n",
    "            index = lat_dfs.index.tolist()\n",
    "            df_out.set_value(index, 'T+{}'.format(i),Matrix[long_val,lat_val])\n",
    "df_out = df_out.drop(columns = ['lat','long','lat_err','long_err','lat_index','long_index',])\n",
    "df_out.to_csv(\"./prediction_output.csv\",index=False)\n",
    "df_out.head(1)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
