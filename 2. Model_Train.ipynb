{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv3D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# Running this notebook requires  the following files and folders:\n",
    "#     1. \"./train_matrix.npy\"\n",
    "#     2. \"./test_matrix.npy\"\n",
    "#     3. \"./full_matrix.npy\"\n",
    "#     4. \"./All_Trained_Models/\"          *folder to store trained models\n",
    "#     5. \"./Top_20_Models/\"               *folder to store trained top 20 models\n",
    "\n",
    "# Running this notebook outputs the following:\n",
    "#     1. \"./All_Trained_Models/\"                         *with all the trained models inside this folder\n",
    "#     2. \"./All_Trained_Models/Result_Score.json\"\n",
    "#     3. \"./All_Trained_Models/Result_Store_Sorted.json\"\n",
    "#     4. \"./Top_20_Models/\"                               *with the top 20 trained models inside this folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gets the input_matrix for training from file\n",
    "input_matrix = np.load('./train_matrix.npy')\n",
    "\n",
    "# Prepare the input and target matrix set for training\n",
    "# Target set matrix is just the input set matrix shifted by one on the time-series axis\n",
    "input_set = input_matrix[:,:-1,:,:,:]\n",
    "target_set = input_matrix[:,1:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the time series data into batches according to day to test out results of different batches\n",
    "# ie, train the model on 1 day = 1 batch or 2 day = 1 batch\n",
    "\n",
    "# This is done in order to iterate and get the best number of days split to produce the best model performance\n",
    "# Model might not train well if the batch sequence length is too short or too long\n",
    "# 61 days is too long to train the LSTM with\n",
    "\n",
    "# If not completely divisible by the days, the remainder time-series data are discarded at the back\n",
    "\n",
    "def generate_batch(input_set,target_set,day_in_batch=1,matrix=input_set):\n",
    "    time_series_batch = int(day_in_batch*24*4)\n",
    "    if day_in_batch == 61:\n",
    "        time_series_batch = time_series_batch - 1\n",
    "    total_df_time_series = matrix.shape[1]\n",
    "    batch_num = int(total_df_time_series/time_series_batch)\n",
    "    input_set_batch = np.zeros((batch_num,time_series_batch,36,46,1))\n",
    "    target_set_batch = np.zeros((batch_num,time_series_batch,36,46,1))\n",
    "\n",
    "    for i in range(batch_num):\n",
    "        input_set_batch[i] = input_set[:,(time_series_batch)*(i):(time_series_batch)*(i+1),:,:,:]\n",
    "        target_set_batch[i] = target_set[:,(time_series_batch)*(i):(time_series_batch)*(i+1),:,:,:]\n",
    "    \n",
    "    return input_set_batch, target_set_batch, batch_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to enable the creation of ConvLSTM model while looping over different parameters\n",
    "def Seq_Model(filter_size=1,layer=1,dropout=0,recurrent_dropout=0):\n",
    "    \n",
    "    seq = Sequential()\n",
    "\n",
    "    for i in range(layer):\n",
    "        seq.add(ConvLSTM2D(filters=filter_size, kernel_size=(3, 3),\n",
    "                           input_shape=(None, 36, 46, 1),padding='same', \n",
    "                           dropout = dropout, \n",
    "                           recurrent_dropout = recurrent_dropout,\n",
    "                           return_sequences=True))\n",
    "        seq.add(BatchNormalization())\n",
    "\n",
    "    seq.add(Conv3D(filters=1, kernel_size=(1, 1, 1), activation='sigmoid',padding='same', data_format='channels_last'))\n",
    "    seq.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network\n",
    "model_filter = [40,60,80]\n",
    "model_layer = [2]\n",
    "day_split = [1,2]\n",
    "epoch = 100\n",
    "dropout = [0, 0.1]\n",
    "recurrent_dropout = [0,0.5,0.8]\n",
    "\n",
    "for filter_size in model_filter:\n",
    "    for layer_depth in model_layer:            \n",
    "        for i,j in enumerate(day_split):\n",
    "            for dropout_num in dropout:\n",
    "                for recurrent_dropout_num in recurrent_dropout:\n",
    "                # Initialise Model\n",
    "                    seq = Seq_Model(filter_size=filter_size,layer=layer_depth,dropout=dropout_num,recurrent_dropout=recurrent_dropout_num)\n",
    "\n",
    "                    # Model Training\n",
    "                    input_set_batch, target_set_batch,batch_num = generate_batch(input_set,target_set,day_in_batch=j,matrix=input_set)\n",
    "                    print(\"Training for {} filter {} layer {} day {} dropout {} rdropout model\".format(filter_size,layer_depth,j,dropout_num,recurrent_dropout_num))\n",
    "                    History = seq.fit(input_set_batch[:batch_num], target_set_batch[:batch_num], batch_size=1,\n",
    "                        epochs=epoch, validation_split=0.1)\n",
    "\n",
    "                    # save model History\n",
    "                    history_dict = History.history\n",
    "                    json.dump(history_dict, open(\"./All_Trained_Models/{}filter{}layer{}day{}drop{}rdrop_model_history.json\".format(filter_size,layer_depth,j,dropout_num,recurrent_dropout_num), 'w'))\n",
    "\n",
    "                    # serialize model to JSON\n",
    "                    model_json = seq.to_json()\n",
    "                    with open(\"./All_Trained_Models/{}filter{}layer{}day{}drop{}rdrop_model.json\".format(filter_size,layer_depth,j,dropout_num,recurrent_dropout_num), \"w\") as json_file:\n",
    "                        json_file.write(model_json)\n",
    "\n",
    "                    # serialize weights to HDF5\n",
    "                    seq.save_weights(\"./All_Trained_Models/{}filter{}layer{}day{}drop{}rdrop_model.h5\".format(filter_size,layer_depth,j,dropout_num,recurrent_dropout_num))\n",
    "                    print(\"Saved {}filter{}layer{}day{}drop{}rdrop_model to disk\".format(filter_size,layer_depth,j,dropout_num,recurrent_dropout_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the RMSE of all the trained models rigorously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the input_matrix for training from file\n",
    "input_matrix = np.load('./test_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmse(T5_actual,T5_pred): \n",
    "    return np.sqrt(np.mean((T5_actual-T5_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline RMSE is the RMSE you get when using the given demand values at T to be values for (T+1 to T+5) \n",
    "# This is the minimum RMSE to beat, as a model is considered useless if just blindly using the last seen values can outperform \n",
    "# the model prediction\n",
    "\n",
    "# The parameters here should be the same as from the training parameters, \n",
    "# as the program will loop through the file to find the models with the given parameters\n",
    "# will have error if file cannot be found.\n",
    "model_layers = [2]\n",
    "days_split = [1,2]\n",
    "model_filter = [40,60,80]\n",
    "dropout = [0, 0.1]\n",
    "rdropout = [0, 0.5, 0.8]\n",
    "\n",
    "T_shift = [0,48,56,76,96,112,130,144,156,187]   # 0 - 187, shifts the test data set window\n",
    "Day_Feed = [14]         # Predict T+5 from x days to test the model prediction across multiple sequences and get the average\n",
    "Overall_min = 1               # Initialise the min RMSE\n",
    "Result_Store = {}\n",
    "\n",
    "# Loops through the files in \"./All_Trained_Models/\" and evaluate the model prediction RMSE performance using test data\n",
    "for model_layer in model_layers:\n",
    "    for i,model_day in enumerate(days_split):\n",
    "        for j,filters in enumerate(model_filter):\n",
    "            for dropout_num in dropout:\n",
    "                for rdropout_num in rdropout:\n",
    "                    # load json and create model\n",
    "                    json_file = open(\"./All_Trained_Models/{}filter{}layer{}day{}drop{}rdrop_model.json\".format(filters,model_layer,model_day,dropout_num,rdropout_num), 'r')\n",
    "                    loaded_model_json = json_file.read()\n",
    "                    json_file.close()\n",
    "                    loaded_model = model_from_json(loaded_model_json)\n",
    "                    # load weights into new model\n",
    "                    loaded_model.load_weights((\"./All_Trained_Models/{}filter{}layer{}day{}drop{}rdrop_model.h5\".format(filters,model_layer,model_day,dropout_num,rdropout_num)))\n",
    "                    # print(\"Loaded model from disk\")\n",
    "\n",
    "                    # \n",
    "                    RMSE_Day_total = 0\n",
    "                    RMSE_Day_total_baseline = 0 # for baseline RMSE----------------\n",
    "                    Max_Day_rmse = 0\n",
    "                    Min_Day_rmse = 1\n",
    "\n",
    "                    Day_all_store = {}\n",
    "                    for day in Day_Feed:\n",
    "                        RMSE_T_shift_total = 0\n",
    "                        Max_T_shift_rmse = 0\n",
    "                        Min_T_shift_rmse = 1\n",
    "                        day_feed = day*24*4\n",
    "\n",
    "                        for a in T_shift:\n",
    "                            input_set = input_matrix[:, a : a + day_feed,:,:,:]\n",
    "                            target_set = input_matrix[:, a : a + day_feed + 5,:,:,:]\n",
    "                            train_14_day = input_set[0][:,:,:,:]\n",
    "\n",
    "                            for k in range(5):\n",
    "                                new_pred = loaded_model.predict(train_14_day[np.newaxis, ::, ::, ::, ::])\n",
    "                                new = new_pred[::, -1, ::, ::, ::]\n",
    "                                train_14_day = np.concatenate((train_14_day, new), axis=0)\n",
    "\n",
    "                            T5_pred = train_14_day[-5:,:,:,:]\n",
    "                            T5_actual = target_set[0][-5:,:,:,:]                    \n",
    "                            rmse = get_rmse(T5_actual,T5_pred)  \n",
    "\n",
    "                            # ----------------------------for baseline RMSE----------------\n",
    "                            T5_baseline = train_14_day[-6:-5,:,:,:]\n",
    "                            baseline = train_14_day[-6:-5,:,:,:]\n",
    "                            for k in range(4):                        \n",
    "                                T5_baseline = np.concatenate((T5_baseline, baseline), axis=0)\n",
    "                            baseline_rmse = get_rmse(T5_actual,T5_baseline)  \n",
    "                            # ----------------------------for baseline RMSE----------------\n",
    "\n",
    "                            if rmse > Max_T_shift_rmse:\n",
    "                                Max_T_shift_rmse = rmse\n",
    "\n",
    "                            if rmse < Min_T_shift_rmse:\n",
    "                                Min_T_shift_rmse = rmse                      \n",
    "\n",
    "                            if rmse > Max_Day_rmse:\n",
    "                                Max_Day_rmse = rmse\n",
    "\n",
    "                            if rmse < Min_Day_rmse:\n",
    "                                Min_Day_rmse = rmse\n",
    "                                filter_min = filters\n",
    "                                layer_min = model_layer\n",
    "                                day_min = model_day\n",
    "\n",
    "                            RMSE_T_shift_total += rmse\n",
    "                            RMSE_Day_total += rmse    \n",
    "                            RMSE_Day_total_baseline += baseline_rmse  # for baseline RMSE----------------\n",
    "                        RMSE_T_shift_avg = RMSE_T_shift_total/len(T_shift)\n",
    "\n",
    "                        Day_store = {}\n",
    "                        Day_store['Avg'] = RMSE_T_shift_avg\n",
    "                        Day_store['Max'] = Max_T_shift_rmse\n",
    "                        Day_store['Min'] = Min_T_shift_rmse\n",
    "\n",
    "                        Day_all_store[day] = Day_store\n",
    "\n",
    "                        Result_Store['{}filter{}layer{}day{}drop{}rdrop'.format(filters,model_layer,model_day,dropout_num,rdropout_num)] = Day_all_store\n",
    "                    RMSE_Day_total_avg = RMSE_Day_total/(len(T_shift)*len(Day_Feed))\n",
    "                    RMSE_Day_total_baseline_avg = RMSE_Day_total_baseline/(len(T_shift)*len(Day_Feed)) # for baseline RMSE----------------\n",
    "\n",
    "                    Model_store = Day_all_store\n",
    "                    Model_store['Avg'] = RMSE_Day_total_avg\n",
    "                    Model_store['Max'] = Max_Day_rmse\n",
    "                    Model_store['Min'] = Min_Day_rmse            \n",
    "\n",
    "                    Result_Store['{}filter{}layer{}day{}drop{}rdrop'.format(filters,model_layer,model_day,dropout_num,rdropout_num)] = Model_store  \n",
    "\n",
    "                    print('Average RMSE for {}filter{}layer{}day{}drop{}rdrop_model is {}. Baseline: {}'.format(filters,model_layer,model_day,dropout_num,rdropout_num,RMSE_Day_total_avg,RMSE_Day_total_baseline_avg))\n",
    "\n",
    "# save Result_Store to JSON\n",
    "json.dump(Result_Store, open(\"./All_Trained_Models/Result_Score.json\", 'w'))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Result_Store\n",
    "Result_Store = json.load(open(\"./All_Trained_Models/Result_Score.json\", 'r')) \n",
    "\n",
    "Result_Store_Sorted = {}\n",
    "i = 1\n",
    "for key, value in sorted(Result_Store.items(), key=lambda tup: (tup[1]['Avg'])):\n",
    "    Model_dict = {}\n",
    "    Model_dict['Model'] = key\n",
    "    Model_dict['Avg'] = value['Avg']\n",
    "    Model_dict['Max'] = value['Max']\n",
    "    Model_dict['Min'] = value['Min']\n",
    "    Result_Store_Sorted['{}'.format(i)] = Model_dict  \n",
    "    i+=1\n",
    "\n",
    "json.dump(Result_Store_Sorted, open(\"./All_Trained_Models/Result_Store_Sorted.json\", 'w'))\n",
    "\n",
    "print(Result_Store_Sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select the Top 20 Models and train on the whole dataset (61 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets the input_matrix for training from file\n",
    "input_matrix = np.load('./full_matrix.npy')\n",
    "\n",
    "# Prepare the input and target matrix set for training\n",
    "# Target set matrix is just the input set matrix shifted by one on the time-series axis\n",
    "input_set = input_matrix[:,:-1,:,:,:]\n",
    "target_set = input_matrix[:,1:,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"./All_Trained_Models/Result_Store_Sorted.json\", 'r')\n",
    "sorted_model = json.loads(file.read())\n",
    "\n",
    "for i in range(20):\n",
    "    i = i + 1\n",
    "    s = sorted_model[str(i)]['Model']\n",
    "    \n",
    "    #extract the 'day' value from the file title\n",
    "    day = float(s[s.find('layer')+len('layer'):s.rfind('day')])\n",
    "    \n",
    "    # load json and create model\n",
    "    json_file = open(\"./All_Trained_Models/{}_model.json\".format(sorted_model[str(i)]['Model']), 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into model\n",
    "    loaded_model.load_weights((\"./All_Trained_Models/{}_model.h5\".format(sorted_model[str(i)]['Model'])))\n",
    "    # print(\"Loaded model from disk\")\n",
    "\n",
    "    input_set_batch, target_set_batch,batch_num = generate_batch(input_set,target_set,day_in_batch=day,matrix=input_set)\n",
    "    loaded_model.compile(loss='mean_squared_error', optimizer='RMSprop')\n",
    "    History = loaded_model.fit(input_set_batch[:batch_num], target_set_batch[:batch_num], batch_size=1, epochs=100, validation_split=0.1)\n",
    "    \n",
    "    # save model History\n",
    "    history_dict = History.history\n",
    "    json.dump(history_dict, open(\"./Top_20_Models/{}_model_history.json\".format(sorted_model[str(i)]['Model']), 'w'))\n",
    "\n",
    "    # serialize model to JSON\n",
    "    model_json = loaded_model.to_json()\n",
    "    with open(\"./Top_20_Models/{}_model.json\".format(sorted_model[str(i)]['Model']), \"w\") as json_file:\n",
    "        json_file.write(model_json)\n",
    "\n",
    "    # serialize weights to HDF5\n",
    "    loaded_model.save_weights(\"./Top_20_Models/{}_model.h5\".format(sorted_model[str(i)]['Model']))\n",
    "    print(\"Saved {}_model to Top_20_Models Folder\".format(sorted_model[str(i)]['Model']))   \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
